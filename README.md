# Insurance-Risk-Scorecard
Insurance Risk Scorecard built using SQL, Python, and Tableau. Includes data pipeline, risk scoring, NTILE tiers, and dashboard.


Below is the **final complete README.md** you can copyâ€“paste **directly into GitHub**.

It is rewritten cleanly, professionally, and optimized for a **portfolio project**.
Just go to:

**Repo â†’ README.md â†’ Edit (pencil icon) â†’ Paste â†’ Commit.**

---

# âœ… **COPYâ€“PASTE THIS ENTIRE README.md INTO GITHUB**

```markdown
# Insurance Risk Scorecard â€” SQL + Python + Tableau

This project builds an **Insurance Risk Scorecard** that ranks policies by risk based on **severity**, **frequency**, and **fraud probability**. It demonstrates an end-to-end analytics workflow combining **Python**, **MySQL**, and **Tableau**.  

The final output includes:
- A complete **risk score** for each policy  
- **Low / Medium / High** risk tiers (NTILE-based)  
- A Tableau dashboard with heatmaps, bar charts, and drilldowns  
- SQL scripts and a reproducible data pipeline

---

## ğŸ¯ Project Objective

Insurance companies require a unified way to compare risks across millions of policies.  
This project answers:

> *â€œWhich policies carry the highest expected risk and should be prioritized for pricing review, fraud investigation, or claims management?â€*

The scorecard compresses three risk dimensions into a single `Final_Score`:

```

Final_Score = 0.4Ã—Severity  +  0.3Ã—Frequency  +  0.3Ã—Fraud_Risk

```

Policies are then categorized into:
- **Tier 1 â€” Low Risk**
- **Tier 2 â€” Medium Risk**
- **Tier 3 â€” High Risk**

---

## ğŸ“ Repository Structure

```

insurance-risk-scorecard/
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ policy_metrics.csv
â”‚   â”œâ”€â”€ policy_risk_scores.csv
â”‚   â””â”€â”€ top100_high_risk.csv
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ scoring.ipynb
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ create_policy_metrics.sql
â”‚   â””â”€â”€ ntile_tiers.sql
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ screenshots/
â”‚       â”œâ”€â”€ dashboard_full.png
â”‚       â”œâ”€â”€ heatmap.png
â”‚       â””â”€â”€ top100_table.png

````

---

## ğŸ§© Data Pipeline Overview

### **1. Raw Data (freMTPL2 Motor Insurance Dataset)**  
Two CSVs were used:
- `freMTPL2freq.csv` â€” policy Ã— claim frequency  
- `freMTPL2sev.csv` â€” policy Ã— average claim severity  

### **2. Python Processing (Jupyter Notebook)**
Steps completed in `scoring.ipynb`:
- Load & merge both CSVs on `policy_id`  
- Fill missing values  
- Create a fraud indicator (1/0)  
- Minâ€“max normalization  
- Compute `Severity_Score`, `Frequency_Score`, `Fraud_Score`  
- Compute weighted `Final_Score`  
- Create percentile-based risk tiers  
- Export:
  - `policy_metrics.csv`
  - `policy_risk_scores.csv`

### **3. SQL Processing (MySQL)**
SQL scripts located in `/sql` perform:

#### **3.1 Aggregation to one row per policy**
```sql
CREATE TABLE policy_metrics_agg AS
SELECT policy_id,
       SUM(frequency) AS frequency,
       AVG(severity) AS severity,
       MAX(is_fraud) AS is_fraud
FROM policy_metrics
GROUP BY policy_id;
````

#### **3.2 Normalized & Scoring Views**

`vw_policy_scores_agg` adds normalized fields.
`vw_risk_tiers_agg` adds `final_score`.

#### **3.3 NTILE(3) Tiers**

```sql
CREATE VIEW vw_tier_ntile_agg AS
SELECT *,
       NTILE(3) OVER (ORDER BY final_score) AS tier_bucket
FROM vw_risk_tiers_agg;
```

### **4. SQL Export of Top High-Risk Policies**

A command-line export retrieves the **top 100 highest-risk policies**:

```
top100_high_risk.csv
```

This is used in Tableau as a drill-down table.

---

## ğŸ“Š Tableau Dashboard

The Tableau dashboard consists of:

### **1. Risk Tier Heatmap**

* Rows: Segments or Tiers
* Columns: Regions or Tiers
* Color: Average Final Score
* Action filter â†’ filters other sheets

### **2. Segment Risk Comparison (Bar Chart)**

Shows average final score per tier or per segment.

### **3. High-Risk Policy Drilldown**

A table built from `top100_high_risk.csv`:

* policy_id
* final_score
* severity
* frequency
* is_fraud

### **4. Dashboard Screenshot**

Screenshots of:

* `dashboard_full.png`
* `heatmap.png`
* `top100_table.png`

> If you publish on Tableau Public, add the link here.

---

## ğŸ“ˆ Key Insights (Example Findings)

* Most policies fall into **Low Risk**, but the top ~2â€“3% contribute disproportionately to severity.
* Policies with repeated claims (high frequency) show strong correlation with fraud indicators.
* Fraud-driven risk is highly concentrated â€” a small group of policies have both high severity and fraud likelihood.
* NTILE-based segmentation effectively separates risk clusters for Management reporting.

---

## ğŸš€ How to Reproduce the Project

### ** Run the Jupyter Notebook**

```
notebooks/scoring.ipynb
```

This will generate:

* `policy_metrics.csv`
* `policy_risk_scores.csv`

### ** Load data into MySQL (optional)**

Run:

```
sql/create_policy_metrics.sql
sql/ntile_tiers.sql
```

### ** Build Tableau Dashboard**

Use:

* `policy_risk_scores.csv`
* `top100_high_risk.csv`

---

## ğŸ§° Tools Used

| Layer           | Tool                     |
| --------------- | ------------------------ |
| Data Processing | Python, Pandas           |
| Storage / SQL   | MySQL                    |
| Visualization   | Tableau Public / Desktop |
| Environment     | Ubuntu, Jupyter Notebook |

---


## ğŸ™‹â€â™‚ï¸ Author

Deepali Sharma
https://www.linkedin.com/in/deepali007

---

```
# END OF README
```
